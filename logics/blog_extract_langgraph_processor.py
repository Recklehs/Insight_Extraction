import os
import re
import json
import traceback
from datetime import datetime, timezone
from typing import List, Literal, Optional

# .env íŒŒì¼ ê´€ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from dotenv import load_dotenv

# MySQL ì—°ë™ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import mysql.connector

# LangChain ë° Google Generative AI ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field

# LangGraph ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langgraph.graph import StateGraph, END

# --- 0. í™˜ê²½ ì„¤ì • ---
# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
load_dotenv()

# í™˜ê²½ ë³€ìˆ˜ í™•ì¸
required_env_vars = ["GOOGLE_API_KEY", "DB_HOST", "DB_USER", "DB_PASSWORD", "DB_NAME"]
for var in required_env_vars:
    if var not in os.environ:
        raise ValueError(f"{var}ê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í™•ì¸í•´ì£¼ì„¸ìš”.")

# ì‚¬ìš©í•  LLM ëª¨ë¸ ì´ë¦„
LLM_MODEL_NAME = "gemini-1.5-flash"


# --- 1. ìµœì¢… ì¶œë ¥ ë° LLM ì‘ë‹µ ìŠ¤í‚¤ë§ˆ ì •ì˜ (Pydantic ëª¨ë¸) ---
class Signal(BaseModel):
    """ì¶”ì¶œëœ ê°œë³„ íˆ¬ì ì‹œê·¸ë„ ì •ë³´ë¥¼ ë‹´ëŠ” ë°ì´í„° êµ¬ì¡°"""
    ticker: str = Field(description="ì‹ë³„ëœ ì£¼ì‹ ë˜ëŠ” ì½”ì¸ í‹°ì»¤ (ì˜ˆ: 'AAPL', 'BTC')")
    action: Literal["buy", "hold", "sell"] = Field(description="ì¶”ì²œ ì•¡ì…˜: 'buy', 'hold', 'sell' ì¤‘ í•˜ë‚˜")
    evidence: str = Field(description="ì¶”ì²œ ì•¡ì…˜ì˜ ê·¼ê±°ê°€ ë˜ëŠ” ì›ë³¸ í…ìŠ¤íŠ¸ ë‚´ì˜ í•µì‹¬ ë¬¸ì¥")
    confidence_score: float = Field(
        description="ì¶”ì²œ ì•¡ì…˜ì— ëŒ€í•œ LLMì˜ ì‹ ë¢°ë„ ì ìˆ˜ (0.0 ~ 1.0)",
        ge=0.0,
        le=1.0
    )

class CoreIdeasResult(BaseModel):
    """í…ìŠ¤íŠ¸ì˜ í•µì‹¬ íˆ¬ì ì•„ì´ë””ì–´ ë¶„ì„ ê²°ê³¼ë¥¼ ë‹´ëŠ” ë°ì´í„° êµ¬ì¡°"""
    is_idea_present: bool = Field(
        description="í…ìŠ¤íŠ¸ì— ì €ìê°€ ì£¼ì¥í•˜ëŠ” ëª…í™•í•œ í•µì‹¬ íˆ¬ì ì•„ì´ë””ì–´ê°€ ì¡´ì¬í•˜ëŠ”ì§€ ì—¬ë¶€ (True/False)"
    )
    core_ideas: List[Signal] = Field(
        description="ì¶”ì¶œëœ ëª¨ë“  í•µì‹¬ íˆ¬ì ì‹œê·¸ë„ì˜ ë¦¬ìŠ¤íŠ¸. is_idea_presentê°€ falseì´ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤."
    )

# --- 2. LangGraph ìƒíƒœ(State) ê°ì²´ ì •ì˜ ---
class GraphState(dict):
    """ê·¸ë˜í”„ì˜ ê° ë…¸ë“œ ê°„ì— ì „ë‹¬ë˜ê³  ì—…ë°ì´íŠ¸ë˜ëŠ” ë°ì´í„° êµ¬ì¡°"""
    original_text: str
    cleaned_text: Optional[str] = None
    results: Optional[List[dict]] = None

# --- 3. ë…¸ë“œ(Node) ë° ì—£ì§€(Edge) í•¨ìˆ˜ ì„¤ê³„ ---
# Gemini LLM ê°ì²´ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
llm = ChatGoogleGenerativeAI(model=LLM_MODEL_NAME, temperature=0)

def preprocess_text(state: GraphState) -> GraphState:
    """ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ ë¶ˆí•„ìš”í•œ ê³µë°±, HTML íƒœê·¸ ë“±ì„ ì œê±°í•©ë‹ˆë‹¤."""
    print("--- 1. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì¤‘ ---")
    original_text = state['original_text']
    cleaned = re.sub(r'<[^>]+>', '', original_text)
    cleaned = re.sub(r'\s+', ' ', cleaned).strip()
    return {"cleaned_text": cleaned}

def extract_core_ideas(state: GraphState) -> GraphState:
    """LLMì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì˜ í•µì‹¬ íˆ¬ì ì•„ì´ë””ì–´ ì—¬ëŸ¬ ê°œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    print("--- 2. í•µì‹¬ íˆ¬ì ì•„ì´ë””ì–´ ì¶”ì¶œ ì¤‘ ---")
    parser = PydanticOutputParser(pydantic_object=CoreIdeasResult)
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", "ë‹¹ì‹ ì€ ì›”ìŠ¤íŠ¸ë¦¬íŠ¸ì˜ ìµœê³  íˆ¬ì ì „ëµê°€ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ê¸´ ê¸€ì—ì„œ ì €ìê°€ ê°€ì¥ ê°•ë ¥í•˜ê²Œ ì£¼ì¥í•˜ëŠ” 'í•µì‹¬ íˆ¬ì ì•„ì´ë””ì–´ë“¤'ì„ ëª¨ë‘ ì •í™•í•˜ê²Œ í¬ì°©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\nì£¼ì–´ì§„ í…ìŠ¤íŠ¸ ì „ì²´ì˜ ë…¼ì§€ë¥¼ íŒŒì•…í•˜ì—¬ ë‹¤ìŒ í•­ëª©ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”:\n1. is_idea_present: ê¸€ì— ëª…í™•í•œ í•µì‹¬ íˆ¬ì ì•„ì´ë””ì–´ê°€ ìˆëŠ”ì§€ ì—¬ë¶€ (True/False)\n2. core_ideas: ì•„ì´ë””ì–´ê°€ ìˆë‹¤ë©´, ê·¸ ëŒ€ìƒ(ticker), ì¶”ì²œ ì•¡ì…˜(action), ê°€ì¥ ê°•ë ¥í•œ ê·¼ê±° ë¬¸ì¥(evidence), ê·¸ë¦¬ê³  ë‹¹ì‹ ì˜ íŒë‹¨ì— ëŒ€í•œ ì‹ ë¢°ë„ ì ìˆ˜(confidence_score)ë¥¼ í¬í•¨í•˜ëŠ” ê°ì²´ë“¤ì˜ 'ë¦¬ìŠ¤íŠ¸'.\në³¸ë¬¸ì˜ í•µì‹¬ ê²°ë¡ ìœ¼ë¡œ ì œì‹œëœ ì£¼ì‹ì´ë‚˜ ì„¹í„°ì— ëŒ€í•œ ì¶”ì²œë§Œ í¬í•¨í•˜ê³ , ë‹¨ìˆœ ë¹„êµ ëŒ€ìƒì´ë‚˜ ë°°ê²½ ì„¤ëª…ìœ¼ë¡œ ì–¸ê¸‰ëœ ì¢…ëª©ì€ ì œì™¸í•˜ì„¸ìš”. ë§Œì•½ ëª…í™•í•œ í•µì‹¬ ì•„ì´ë””ì–´ê°€ ì—†ë‹¤ë©´ 'is_idea_present'ë¥¼ falseë¡œ ì„¤ì •í•˜ê³  'core_ideas'ë¥¼ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“œì„¸ìš”.\në°˜ë“œì‹œ ì§€ì •ëœ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.\n\n{format_instructions}"),
            ("human", "ë¶„ì„í•  í…ìŠ¤íŠ¸:\n```{text}```"),
        ]
    ).partial(format_instructions=parser.get_format_instructions())

    chain = prompt | llm | parser
    cleaned_text = state['cleaned_text']
    response = chain.invoke({"text": cleaned_text})

    if response.is_idea_present and response.core_ideas:
        print(f"   > âœ… í•µì‹¬ ì•„ì´ë””ì–´ {len(response.core_ideas)}ê°œ ë°œê²¬!")
        results_as_dicts = [idea.model_dump() for idea in response.core_ideas]
        print(f"   > ì¶”ì¶œëœ ì‹œê·¸ë„: {json.dumps(results_as_dicts, indent=2, ensure_ascii=False)}")
        return {"results": results_as_dicts}
    else:
        print("   > âŒ ëª…í™•í•œ í•µì‹¬ íˆ¬ì ì•„ì´ë””ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return {"results": []}

# --- 4. ê·¸ë˜í”„ êµ¬ì„± ë° ì»´íŒŒì¼ ---
workflow = StateGraph(GraphState)
workflow.add_node("preprocess_text", preprocess_text)
workflow.add_node("extract_core_ideas", extract_core_ideas)
workflow.set_entry_point("preprocess_text")
workflow.add_edge("preprocess_text", "extract_core_ideas")
workflow.add_edge("extract_core_ideas", END)
app = workflow.compile()


# --- 5. ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™ ë° ì²˜ë¦¬ ë¡œì§ ---
def get_db_connection():
    """ë°ì´í„°ë² ì´ìŠ¤ ì»¤ë„¥ì…˜ì„ ìƒì„±í•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        conn = mysql.connector.connect(
            host=os.environ["DB_HOST"],
            user=os.environ["DB_USER"],
            password=os.environ["DB_PASSWORD"],
            database=os.environ["DB_NAME"]
        )
        print("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ")
        return conn
    except mysql.connector.Error as e:
        print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {e}")
        raise

def process_and_store_item(conn, crawl_item):
    """ë‹¨ì¼ í¬ë¡¤ë§ ì•„ì´í…œì„ ë¶„ì„í•˜ê³  ê²°ê³¼ë¥¼ DBì— ì €ì¥í•©ë‹ˆë‹¤."""
    cursor = conn.cursor()
    crawl_guid = crawl_item[0]
    crawl_text = crawl_item[1]
    log_id = None

    try:
        # 1. processing_logsì— ë¶„ì„ ì‹œì‘ ê¸°ë¡ ì‚½ì…
        log_insert_query = """
        INSERT INTO processing_logs (crawl_result_guid, status, llm_model, processed_at)
        VALUES (%s, %s, %s, %s)
        """
        # ì´ˆê¸° ìƒíƒœëŠ” 'failed'ë¡œ ì„¤ì •í•˜ê³  ì„±ê³µ ì‹œ 'success'ë¡œ ì—…ë°ì´íŠ¸
        cursor.execute(log_insert_query, (crawl_guid, 'failed', LLM_MODEL_NAME, datetime.now(timezone.utc)))
        log_id = cursor.lastrowid
        print(f"ğŸ“˜ [{crawl_guid}] ë¶„ì„ ì‹œì‘. Log ID: {log_id}")

        # 2. LangGraph íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        inputs = {"original_text": crawl_text}
        final_state = app.invoke(inputs)
        signals = final_state.get("results", [])

        # 3. investment_signalsì— ê²°ê³¼ ì €ì¥
        if signals:
            signal_insert_query = """
            INSERT INTO investment_signals (log_id, ticker, action, evidence, confidence_score)
            VALUES (%s, %s, %s, %s, %s)
            """
            signal_data = [
                (log_id, s['ticker'], s['action'], s['evidence'], s['confidence_score'])
                for s in signals
            ]
            cursor.executemany(signal_insert_query, signal_data)
            print(f"   > ğŸ“ˆ ì‹œê·¸ë„ {len(signals)}ê°œ ì €ì¥ ì™„ë£Œ.")

        # 4. processing_logs ìƒíƒœ 'success'ë¡œ ì—…ë°ì´íŠ¸
        cursor.execute("UPDATE processing_logs SET status = 'success' WHERE log_id = %s", (log_id,))

        # 5. crawl_result ìƒíƒœ 'is_processed = 1'ë¡œ ì—…ë°ì´íŠ¸
        cursor.execute("UPDATE crawl_result SET is_processed = 1, processed_at = %s WHERE guid = %s", (datetime.now(timezone.utc), crawl_guid))
        
        conn.commit()
        print(f"âœ… [{crawl_guid}] ëª¨ë“  ì²˜ë¦¬ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ.")

    except Exception as e:
        print(f"âŒ [{crawl_guid}] ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        # ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ processing_logsì— ê¸°ë¡
        if log_id:
            error_message = traceback.format_exc()
            cursor.execute("UPDATE processing_logs SET error_message = %s WHERE log_id = %s", (error_message, log_id))
        conn.rollback() # ì—ëŸ¬ ë°œìƒ ì‹œ ëª¨ë“  ë³€ê²½ì‚¬í•­ ë¡¤ë°±
    finally:
        cursor.close()

def main_processor():
    """
    Airflowì—ì„œ í˜¸ì¶œí•  ë©”ì¸ í•¨ìˆ˜.
    DBì—ì„œ ì²˜ë¦¬ë˜ì§€ ì•Šì€ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ ë¶„ì„í•˜ê³  ì €ì¥í•©ë‹ˆë‹¤.
    """
    conn = None
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # is_processedê°€ 0ì¸ ë°ì´í„° ì¡°íšŒ
        cursor.execute("SELECT guid, crawled_text FROM crawl_result WHERE is_processed = 0")
        items_to_process = cursor.fetchall()
        
        print(f"ğŸš€ ì´ {len(items_to_process)}ê°œì˜ ìƒˆë¡œìš´ í•­ëª©ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        
        for item in items_to_process:
            process_and_store_item(conn, item)
            
        cursor.close()
        print("ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ.")

    except Exception as e:
        print(f"ğŸ”¥ ë©”ì¸ í”„ë¡œì„¸ì„œì—ì„œ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        if conn and conn.is_connected():
            conn.close()
            print("ğŸšª ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¢…ë£Œ.")


# --- 6. ì§ì ‘ ì‹¤í–‰ì„ ìœ„í•œ ìŠ¤í¬ë¦½íŠ¸ ---
if __name__ == "__main__":
    main_processor()
